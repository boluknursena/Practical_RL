{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  FrozenLake\n",
    "Today you are going to learn how to survive walking over the (virtual) frozen lake through discrete optimization.\n",
    "\n",
    "<img src=\"http://vignette2.wikia.nocookie.net/riseoftheguardians/images/4/4c/Jack's_little_sister_on_the_ice.jpg/revision/latest?cb=20141218030206\" alt=\"a random image to attract attention\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-22 19:09:53,726] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "#start new game\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# display the game state\n",
    "# env.step(action_to_i['right'])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### legend\n",
    "\n",
    "![img](https://cdn-images-1.medium.com/max/800/1*MCjDzR-wfMMkS0rPqXSmKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym interface\n",
    "\n",
    "The three main methods of an environment are\n",
    "* __reset()__ - reset environment to initial state, _return first observation_\n",
    "* __render()__ - show current environment state (a more colorful version :) )\n",
    "* __step(a)__ - commit action __a__ and return (new observation, reward, is done, info)\n",
    " * _new observation_ - an observation right after commiting the action __a__\n",
    " * _reward_ - a number representing your reward for commiting action __a__\n",
    " * _is done_ - True if the MDP has just finished, False if still in progress\n",
    " * _info_ - some auxilary stuff about what just happened. Ignore it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation code: 0\n",
      "printing observation:\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "observations: Discrete(16) n= 16\n",
      "actions: Discrete(4) n= 4\n"
     ]
    }
   ],
   "source": [
    "print(\"initial observation code:\", env.reset())\n",
    "print('printing observation:')\n",
    "env.render()\n",
    "print(\"observations:\", env.observation_space, 'n=', env.observation_space.n)\n",
    "print(\"actions:\", env.action_space, 'n=', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking action 2 (right)\n",
      "new observation code: 4\n",
      "reward: 0.0\n",
      "is game over?: False\n",
      "printing new state:\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "print(\"taking action 2 (right)\")\n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n",
    "print(\"printing new state:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_to_i = {\n",
    "    'left':0,\n",
    "    'down':1,\n",
    "    'right':2,\n",
    "    'up':3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with it\n",
    "* Try walking 5 steps without falling to the (H)ole\n",
    " * Bonus quest - get to the (G)oal\n",
    "* Sometimes your actions will not be executed properly due to slipping over ice\n",
    "* If you fall, call __env.reset()__ to restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(action_to_i['up'])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: random search (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "* The environment has a 4x4 grid of states (16 total), they are indexed from 0 to 15\n",
    "* From each states there are 4 actions (left,down,right,up), indexed from 0 to 3\n",
    "\n",
    "We need to define agent's policy of picking actions given states. Since we have only 16 disttinct states and 4 actions, we can just store the action for each state in an array.\n",
    "\n",
    "This basically means that any array of 16 integers from 0 to 3 makes a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "def get_random_policy():\n",
    "    \"\"\"\n",
    "    Build a numpy array representing agent policy.\n",
    "    This array must have one element per each of 16 environment states.\n",
    "    Element must be an integer from 0 to 3, representing action\n",
    "    to take from that state.\n",
    "    \"\"\"\n",
    "    return np.random.randint(n_actions, size=n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 2, 3, 1, 3, 1, 2, 0, 0, 0])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action frequencies over 10^4 samples: [ 0.25014375  0.25130625  0.2495375   0.2490125 ]\n",
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [get_random_policy() for i in range(10**4)]\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should match n_actions-1'\n",
    "action_probas = np.unique(policies, return_counts=True)[-1] /10**4. /n_states\n",
    "print(\"Action frequencies over 10^4 samples:\",action_probas)\n",
    "assert np.allclose(action_probas, [1. / n_actions] * n_actions, atol=0.05), \"The policies aren't uniformly random (maybe it's just an extremely bad luck)\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate!\n",
    "* Implement a simple function that runs one game and returns the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_reward(env, policy, t_max=100):\n",
    "    \"\"\"\n",
    "    Interact with an environment, return sum of all rewards.\n",
    "    If game doesn't end on t_max (e.g. agent walks into a wall), \n",
    "    force end the game and return whatever reward you got so far.\n",
    "    Tip: see signature of env.step(...) method above.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    for i in range(t_max):\n",
    "        s, r, done, info = env.step(policy[s])\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "policy = get_random_policy()\n",
    "sample_reward(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(get_random_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 10^3 sessions...\n",
      "Looks good!\n"
     ]
    }
   ],
   "source": [
    "print(\"generating 10^3 sessions...\")\n",
    "rewards = [sample_reward(env,get_random_policy()) for _ in range(10**3)]\n",
    "assert all([type(r) in (int, float) for r in rewards]), 'sample_reward must return a single number'\n",
    "assert all([0 <= r <= 1 for r in rewards]), 'total rewards should be between 0 and 1 for frozenlake (if solving taxi, delete this line)'\n",
    "print(\"Looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(env, policy, n_times=100):\n",
    "    \"\"\"Run several evaluations and average the score the policy gets.\"\"\"\n",
    "    rewards = [sample_reward(env, policy) for i in range(n_times)]\n",
    "    return float(np.mean(rewards))\n",
    "evaluate(env, get_random_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random policy:\n",
      "< v < >\n",
      "v H < H\n",
      "v < ^ H\n",
      "H > < G\n"
     ]
    }
   ],
   "source": [
    "def print_policy(policy):\n",
    "    \"\"\"a function that displays a policy in a human-readable way.\"\"\"\n",
    "    lake = \"SFFFFHFHFFFHHFFG\"\n",
    "    assert env.spec.id == \"FrozenLake-v0\", \"this function only works with frozenlake 4x4\"\n",
    "\n",
    "    \n",
    "    # where to move from each tile (we're a bit unsure if this is accurate)\n",
    "    arrows = ['>^v<'[a] for a in policy]\n",
    "    \n",
    "    #draw arrows above S and F only\n",
    "    signs = [arrow if tile in \"SF\" else tile for arrow, tile in zip(arrows, lake)]\n",
    "    \n",
    "    for i in range(0, 16, 4):\n",
    "        print(' '.join(signs[i:i+4]))\n",
    "\n",
    "print(\"random policy:\")\n",
    "print_policy(get_random_policy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.12\n",
      "Best policy:\n",
      "^ < > >\n",
      "> H ^ H\n",
      "< ^ < H\n",
      "H v v G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                         | 1/1000 [00:00<02:41,  6.17it/s]\n",
      "  0%|                                         | 3/1000 [00:00<02:09,  7.68it/s]\n",
      "  0%|▏                                        | 5/1000 [00:00<01:58,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.15\n",
      "Best policy:\n",
      "< < v >\n",
      "v H ^ H\n",
      "< > ^ H\n",
      "H v ^ G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▎                                        | 7/1000 [00:00<01:44,  9.52it/s]\n",
      "  1%|▎                                        | 9/1000 [00:00<01:29, 11.11it/s]\n",
      "  1%|▍                                       | 11/1000 [00:00<01:23, 11.90it/s]\n",
      "  1%|▌                                       | 13/1000 [00:01<01:21, 12.12it/s]\n",
      "  2%|▋                                       | 16/1000 [00:01<01:07, 14.52it/s]\n",
      "  2%|▊                                       | 19/1000 [00:01<00:58, 16.63it/s]\n",
      "  2%|▊                                       | 21/1000 [00:01<01:00, 16.23it/s]\n",
      "  2%|▉                                       | 23/1000 [00:01<00:56, 17.20it/s]\n",
      "  2%|█                                       | 25/1000 [00:01<00:54, 17.76it/s]\n",
      "  3%|█                                       | 28/1000 [00:01<00:50, 19.11it/s]\n",
      "  3%|█▏                                      | 31/1000 [00:01<00:48, 20.19it/s]\n",
      "  3%|█▎                                      | 34/1000 [00:01<00:44, 21.84it/s]\n",
      "  4%|█▍                                      | 37/1000 [00:02<00:58, 16.54it/s]\n",
      "  4%|█▌                                      | 39/1000 [00:02<01:03, 15.04it/s]\n",
      "  4%|█▋                                      | 41/1000 [00:02<01:01, 15.71it/s]\n",
      "  4%|█▋                                      | 43/1000 [00:02<01:53,  8.46it/s]\n",
      "  5%|█▊                                      | 46/1000 [00:03<01:34, 10.05it/s]\n",
      "  5%|█▉                                      | 49/1000 [00:03<01:16, 12.38it/s]\n",
      "  5%|██                                      | 51/1000 [00:03<01:09, 13.72it/s]\n",
      "  5%|██▏                                     | 54/1000 [00:03<00:58, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.18\n",
      "Best policy:\n",
      "^ > < v\n",
      "> H v H\n",
      "< ^ > H\n",
      "H < v G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|██▎                                     | 57/1000 [00:03<01:03, 14.89it/s]\n",
      "  6%|██▎                                     | 59/1000 [00:03<01:02, 15.10it/s]\n",
      "  6%|██▍                                     | 61/1000 [00:03<00:58, 16.14it/s]\n",
      "  6%|██▌                                     | 63/1000 [00:04<00:54, 17.13it/s]\n",
      "  6%|██▌                                     | 65/1000 [00:04<01:04, 14.47it/s]\n",
      "  7%|██▋                                     | 68/1000 [00:04<01:29, 10.46it/s]\n",
      "  7%|██▊                                     | 70/1000 [00:04<01:27, 10.65it/s]\n",
      "  7%|██▉                                     | 73/1000 [00:05<01:12, 12.86it/s]\n",
      "  8%|███                                     | 76/1000 [00:05<01:02, 14.67it/s]\n",
      "  8%|███                                     | 78/1000 [00:05<01:07, 13.69it/s]\n",
      "  8%|███▏                                    | 80/1000 [00:05<01:02, 14.75it/s]\n",
      "  8%|███▎                                    | 83/1000 [00:05<00:56, 16.21it/s]\n",
      "  9%|███▍                                    | 86/1000 [00:05<00:51, 17.67it/s]\n",
      "  9%|███▌                                    | 89/1000 [00:05<00:48, 18.69it/s]\n",
      "  9%|███▋                                    | 92/1000 [00:06<00:50, 18.12it/s]\n",
      " 10%|███▊                                    | 95/1000 [00:06<00:47, 19.14it/s]\n",
      " 10%|███▉                                    | 97/1000 [00:06<00:46, 19.33it/s]\n",
      " 10%|███▉                                   | 100/1000 [00:06<00:44, 20.32it/s]\n",
      " 10%|████                                   | 103/1000 [00:06<00:46, 19.39it/s]\n",
      " 11%|████▏                                  | 106/1000 [00:06<00:41, 21.55it/s]\n",
      " 11%|████▎                                  | 109/1000 [00:06<00:39, 22.28it/s]\n",
      " 11%|████▎                                  | 112/1000 [00:06<00:41, 21.22it/s]\n",
      " 12%|████▍                                  | 115/1000 [00:07<00:41, 21.55it/s]\n",
      " 26%|██████████▏                            | 262/1000 [00:13<00:29, 24.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.49\n",
      "Best policy:\n",
      "> < > ^\n",
      "> H > H\n",
      "< ^ > H\n",
      "H < ^ G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1000/1000 [00:50<00:00, 19.73it/s]\n"
     ]
    }
   ],
   "source": [
    "best_policy = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    policy = get_random_policy()\n",
    "    score = evaluate(env, policy)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_policy = policy\n",
    "        print(\"New best score:\", score)\n",
    "        print(\"Best policy:\")\n",
    "        print_policy(best_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II Genetic algorithm (4 points)\n",
    "\n",
    "The next task is to devise some more efficient way to perform policy search.\n",
    "We'll do that with a bare-bones evolutionary algorithm.\n",
    "[unless you're feeling masochistic and wish to do something entirely different which is bonus points if it works]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossover(policy1, policy2, p=0.5):\n",
    "    \"\"\"\n",
    "    for each state, with probability p take action from policy1, else policy2\n",
    "    \"\"\" \n",
    "    return [policy1[i] if np.random.random() <= p else policy2[i] for i in range(len(policy1)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutation(policy, p=0.1):\n",
    "    \"\"\"\n",
    "    for each state, with probability p replace action with random action\n",
    "    Tip: mutation can be written as crossover with random policy\n",
    "    \"\"\"\n",
    "    return crossover(get_random_policy(), policy, p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "policies = [crossover(get_random_policy(), get_random_policy()) \n",
    "            for i in range(10**4)]\n",
    "\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should be n_actions-1'\n",
    "\n",
    "assert any([np.mean(crossover(np.zeros(n_states), np.ones(n_states))) not in (0, 1)\n",
    "               for _ in range(100)]), \"Make sure your crossover changes each action independently\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 30 #how many cycles to make\n",
    "pool_size = 100 #how many policies to maintain\n",
    "n_crossovers = 50 #how many crossovers to make on each step\n",
    "n_mutations = 50 #how many mutations to make on each tick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing...\n",
      "Epoch 0:\n",
      "best score: 0.19\n",
      "> v v <\n",
      "> H v H\n",
      "< v v H\n",
      "H < v G\n",
      "Epoch 1:\n",
      "best score: 0.21\n",
      "^ ^ v <\n",
      "> H ^ H\n",
      "< ^ > H\n",
      "H v v G\n",
      "Epoch 2:\n",
      "best score: 0.29\n",
      "> < ^ ^\n",
      "> H v H\n",
      "< ^ > H\n",
      "H > v G\n",
      "Epoch 3:\n",
      "best score: 0.35\n",
      "> < < <\n",
      "> H v H\n",
      "< v v H\n",
      "H v v G\n",
      "Epoch 4:\n",
      "best score: 0.59\n",
      "> < < <\n",
      "> H ^ H\n",
      "< ^ > H\n",
      "H v v G\n",
      "Epoch 5:\n",
      "best score: 0.71\n",
      "> ^ < >\n",
      "> H ^ H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "Epoch 6:\n",
      "best score: 0.71\n",
      "> < ^ v\n",
      "> H ^ H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "Epoch 7:\n",
      "best score: 0.76\n",
      "> > < v\n",
      "> H > H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "Epoch 8:\n",
      "best score: 0.82\n",
      "> ^ v >\n",
      "> H v H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "Epoch 9:\n",
      "best score: 0.82\n",
      "> ^ > <\n",
      "> H v H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "Epoch 10:\n",
      "best score: 0.78\n",
      "> < < <\n",
      "> H v H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "Epoch 11:\n",
      "best score: 0.8\n",
      "> < > >\n",
      "> H > H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "Epoch 12:\n",
      "best score: 0.85\n",
      "> < > <\n",
      "> H v H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "Epoch 13:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-393-e264ef0f9d40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#add new policies to the pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcrossovered\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmutated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mpool_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#select pool_size best policies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-393-e264ef0f9d40>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#add new policies to the pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcrossovered\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmutated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mpool_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#select pool_size best policies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-386-3d1488f5de0f>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(env, policy, n_times)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Run several evaluations and average the score the policy gets.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_random_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-386-3d1488f5de0f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Run several evaluations and average the score the policy gets.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_random_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-383-4251b8e6315b>\u001b[0m in \u001b[0;36msample_reward\u001b[1;34m(env, policy, t_max)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[1;34m(prob_n, np_random)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"initializing...\")\n",
    "pool = [get_random_policy() for i in range(pool_size)]\n",
    "pool_scores = [evaluate(env, policy) for policy in pool]\n",
    "\n",
    "\n",
    "assert type(pool) == type(pool_scores) == list\n",
    "assert len(pool) == len(pool_scores) == pool_size\n",
    "assert all([type(score) in (float, int) for score in pool_scores])\n",
    "\n",
    "\n",
    "#main loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch %s:\"%epoch)\n",
    "\n",
    "    crossovered = [crossover(pool[np.random.randint(len(pool))], pool[np.random.randint(len(pool))], p=0.3) for i in range(n_crossovers)]\n",
    "    mutated = [mutation(pol_cros, p=0.3) for pol_cros in crossovered]\n",
    "\n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "\n",
    "    #add new policies to the pool\n",
    "    pool += crossovered + mutated\n",
    "    pool_scores = [evaluate(env, policy) for policy in pool]\n",
    "\n",
    "    #select pool_size best policies\n",
    "    selected_indices = np.argsort(pool_scores)[-pool_size:]\n",
    "    pool = [pool[i] for i in selected_indices][-50:]\n",
    "    pool_scores = [pool_scores[i] for i in selected_indices]\n",
    "\n",
    "    #print the best policy so far (last in ascending score order)\n",
    "    print(\"best score:\", pool_scores[-1])\n",
    "    print_policy(pool[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## moar\n",
    "\n",
    "The parameters of the genetic algorithm aren't optimal, try to find something better. (size, crossovers and mutations)\n",
    "\n",
    "Try alternative crossover and mutation strategies\n",
    "* prioritize crossover for higher-scorers?\n",
    "* try to select a more diverse pool, not just best scorers?\n",
    "* Just tune the f*cking probabilities.\n",
    "\n",
    "See which combination works best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III (4 points +)\n",
    "\n",
    "The frozenlake problem above is just too simple: you can beat it even with a random policy search. Go solve something more complicated.\n",
    "\n",
    "Pick __one of the two tasks__:\n",
    "\n",
    "* __FrozenLake8x8-v0__ - frozenlake big brother. Achieve score >0.7\n",
    "* __Taxi-v1__ - essentially a maze where you get score for moving passengers to their destinations. Achieve score >-100)\n",
    "\n",
    "Your homework assignment is beating that score (see tips below).\n",
    "\n",
    "\n",
    "### Some tips:\n",
    "* When solving those envs, please make sure your t_max is large enough to finish game with suboptimal policy. For example, __Taxi-v0 only trains if you let it play for 10k+ ticks/session__. For frozenlake8x8 it's less dire.\n",
    "* Random policy search is worth trying as a sanity check, but in general you should expect the genetic algorithm (or anything you devised in it's place) to fare much better that random.\n",
    "* While _it's okay to adapt the tabs above to your chosen env_, make sure you didn't hard-code any constants there (e.g. 16 states or 4 actions).\n",
    "* `print_policy` function was built for the frozenlake-v0 env so it will break on any other env. You could simply ignore it or rewrite it for your env.\n",
    "* in function `sample_reward`, __make sure t_max steps is enough to solve the environment__ even if agent is sometimes acting suboptimally. To estimate that, run several sessions without time limit and measure their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-22 19:16:47,564] Making new env: FrozenLake8x8-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "def get_random_policy():\n",
    "    \"\"\"\n",
    "    Build a numpy array representing agent policy.\n",
    "    This array must have one element per each of 16 environment states.\n",
    "    Element must be an integer from 0 to 3, representing action\n",
    "    to take from that state.\n",
    "    \"\"\"\n",
    "    return np.random.randint(n_actions, size=n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 1, 3, 0, 0, 2, 0, 3, 2, 3, 3, 0, 0, 1, 3, 0, 3, 1, 1, 1, 2, 0,\n",
       "       2, 0, 1, 0, 0, 1, 3, 0, 3, 3, 1, 0, 0, 0, 2, 0, 2, 1, 2, 3, 1, 0, 1,\n",
       "       1, 1, 3, 3, 1, 3, 3, 3, 0, 3, 1, 2, 3, 3, 1, 3, 1, 2])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random policy:\n",
      "v < < ^ > ^ > >\n",
      "v > < v < < v >\n",
      "^ ^ < H > < ^ ^\n",
      "^ > ^ > < H ^ ^\n",
      "v ^ > H < > ^ ^\n",
      "v H H > > v H <\n",
      "v H < > H > H ^\n",
      "> v < H < < ^ G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 1, 1, 2, 1, 3, 2, 2, 3, 0, 3, 1, 3, 2, 2, 3, 0, 0, 1, 2, 1,\n",
       "       2, 2, 2, 3, 2, 2, 1, 1, 0, 1, 2, 0, 2, 0, 1, 1, 1, 2, 0, 0, 1, 0, 3,\n",
       "       0, 1, 2, 3, 2, 2, 3, 2, 2, 3, 0, 2, 0, 3, 0, 3, 3, 1])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_policy(policy):\n",
    "    \"\"\"a function that displays a policy in a human-readable way.\"\"\"\n",
    "    lake = \"SFFFFFFFFFFFFFFFFFFHFFFFFFFFFHFFFFFHFFFFFHHFFFHFFHFFHFHFFFFHFFFG\"\n",
    "    \n",
    "    # where to move from each tile (we're a bit unsure if this is accurate)\n",
    "    arrows = ['>^v<'[a] for a in policy]\n",
    "    \n",
    "    #draw arrows above S and F only\n",
    "    signs = [arrow if tile in \"SF\" else tile for arrow, tile in zip(arrows, lake)]\n",
    "    \n",
    "    for i in range(0, 64, 8):\n",
    "        print(' '.join(signs[i:i+8]))\n",
    "\n",
    "print(\"random policy:\")\n",
    "print_policy(get_random_policy())\n",
    "get_random_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30 #how many cycles to make\n",
    "pool_size = 100 #how many policies to maintain\n",
    "n_crossovers = 50 #how many crossovers to make on each step\n",
    "n_mutations = 50 #how many mutations to make on each tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing...\n",
      "Epoch 0:\n",
      "best score: 0.05\n",
      "> ^ ^ ^ < v v >\n",
      "< ^ ^ < > < v ^\n",
      "v ^ > H > v ^ v\n",
      "< ^ v v ^ H v v\n",
      "< v v H ^ v < v\n",
      "v H H > ^ < H v\n",
      "v H < ^ H ^ H ^\n",
      "< < > H v v < G\n",
      "Epoch 1:\n",
      "best score: 0.06\n",
      "< ^ ^ ^ v > v >\n",
      "< ^ ^ < v < v v\n",
      "v ^ > H ^ > > v\n",
      "< > v v ^ H v v\n",
      "< v v H ^ v v v\n",
      "v H H > ^ ^ H v\n",
      "< H ^ < H < H ^\n",
      "v > > H v ^ > G\n",
      "Epoch 2:\n",
      "best score: 0.12\n",
      "> < ^ ^ < v > v\n",
      "< ^ v v v v v v\n",
      "v ^ > H > < ^ v\n",
      "< ^ v < > H v v\n",
      "< < v H v v < v\n",
      "^ H H > > ^ H v\n",
      "^ H < v H ^ H ^\n",
      "^ ^ > H < v ^ G\n",
      "Epoch 3:\n",
      "best score: 0.1\n",
      "^ v < v ^ v ^ >\n",
      "< < v v v ^ < ^\n",
      "v v > H v < ^ v\n",
      "v v ^ < ^ H < v\n",
      "< > < H v > > ^\n",
      "v H H < ^ < H v\n",
      "v H ^ ^ H < H >\n",
      "< ^ ^ H > v ^ G\n",
      "Epoch 4:\n",
      "best score: 0.17\n",
      "> v v v > ^ v <\n",
      "^ ^ v < ^ ^ v >\n",
      "< v < H v v v ^\n",
      "> v > > > H v ^\n",
      "> > ^ H < ^ < v\n",
      "v H H > ^ < H v\n",
      "v H < ^ H ^ H v\n",
      "< < ^ H < > v G\n",
      "Epoch 5:\n",
      "best score: 0.19\n",
      "^ v < < < v > <\n",
      "< < v < v ^ < ^\n",
      "> ^ v H v < ^ v\n",
      "< ^ v > > H v ^\n",
      "> > v H > > > v\n",
      "> H H v v < H v\n",
      "v H v < H > H v\n",
      "> ^ ^ H v ^ < G\n",
      "Epoch 6:\n",
      "best score: 0.32\n",
      "> v v v < < v >\n",
      "< < v < v v v ^\n",
      "v ^ > H v < ^ v\n",
      "< ^ v > < H v v\n",
      "< ^ v H < > < v\n",
      "^ H H > ^ ^ H v\n",
      "^ H < > H ^ H v\n",
      "^ ^ > H v v v G\n",
      "Epoch 7:\n",
      "best score: 0.29\n",
      "> v v v < < v >\n",
      "< < v < v v v ^\n",
      "v ^ > H v < ^ v\n",
      "< ^ v > < H v v\n",
      "< ^ v H < > < v\n",
      "^ H H > ^ ^ H v\n",
      "^ H < > H ^ H v\n",
      "^ ^ > H v v v G\n",
      "Epoch 8:\n",
      "best score: 0.32\n",
      "> v v v < < v >\n",
      "< < v < v v v ^\n",
      "v v > H v < ^ v\n",
      "< ^ ^ > < H v ^\n",
      "< > < H < > < v\n",
      "^ H H > ^ < H v\n",
      "v H ^ ^ H ^ H v\n",
      "^ ^ > H v < ^ G\n",
      "Epoch 9:\n",
      "best score: 0.32\n",
      "> v < v ^ > v v\n",
      "< < < < v v v v\n",
      "v ^ > H v < ^ v\n",
      "^ < ^ > < H ^ v\n",
      "< > < H < > < v\n",
      "v H H > ^ ^ H v\n",
      "v H ^ ^ H > H v\n",
      "< < ^ H ^ > < G\n",
      "Epoch 10:\n",
      "best score: 0.39\n",
      "> v v v < < v >\n",
      "< < v < v v v ^\n",
      "v ^ > H v < ^ v\n",
      "< ^ v > < H v v\n",
      "< ^ v H < > < v\n",
      "^ H H > ^ ^ H v\n",
      "^ H < > H ^ H v\n",
      "^ ^ > H v v v G\n",
      "Epoch 11:\n",
      "best score: 0.37\n",
      "< v ^ v < < v >\n",
      "^ < < < v v v ^\n",
      "< v > H < < ^ v\n",
      "< < > < < H < v\n",
      "< ^ < H < > < v\n",
      "v H H > > ^ H v\n",
      "v H > v H ^ H v\n",
      "< ^ v H v v > G\n",
      "Epoch 12:\n",
      "best score: 0.37\n",
      "^ v < < < v v >\n",
      "< < v < v v v ^\n",
      "v v < H > < v v\n",
      "> < v < > H v v\n",
      "> > v H > > < v\n",
      "v H H ^ > v H v\n",
      "v H < ^ H v H v\n",
      "< ^ > H < < > G\n",
      "Epoch 13:\n",
      "best score: 0.46\n",
      "> v v v < < v >\n",
      "< < < < v ^ v ^\n",
      "v ^ < H v < v v\n",
      "^ ^ v < < H v v\n",
      "< > ^ H > > < v\n",
      "> H H > ^ ^ H v\n",
      "^ H < > H ^ H v\n",
      "^ ^ > H v > ^ G\n",
      "Epoch 14:\n",
      "best score: 0.52\n",
      "v v v ^ < < v v\n",
      "< < < < v v v ^\n",
      "v ^ > H v < v v\n",
      "< ^ v v > H v v\n",
      "> ^ < H > > < v\n",
      "v H H > > < H v\n",
      "v H < > H v H v\n",
      "< ^ > H > v > G\n",
      "Epoch 15:\n",
      "best score: 0.48\n",
      "v v v v < v v >\n",
      "^ < < < v v v v\n",
      "v v > H < < v ^\n",
      "< < > < > H v ^\n",
      "> ^ < H < ^ < v\n",
      "> H H > ^ v H v\n",
      "v H v < H ^ H v\n",
      "^ ^ ^ H < v > G\n",
      "Epoch 16:\n",
      "best score: 0.57\n",
      "^ v < v ^ < v v\n",
      "< < < < v v v ^\n",
      "v ^ v H v < v v\n",
      "< ^ v > > H v ^\n",
      "> > v H > > < v\n",
      "v H H > > < H v\n",
      "v H v > H v H v\n",
      "< ^ ^ H v v < G\n",
      "Epoch 17:\n",
      "best score: 0.58\n",
      "v ^ v v ^ < v v\n",
      "< < < < v v v ^\n",
      "v v > H v < v v\n",
      "< < v v > H v v\n",
      "> > v H > > < v\n",
      "> H H ^ ^ < H v\n",
      "v H ^ ^ H ^ H v\n",
      "< ^ > H v < ^ G\n",
      "Epoch 18:\n",
      "best score: 0.54\n",
      "v v < v < < v v\n",
      "< < < < v v v ^\n",
      "< ^ < H v < v v\n",
      "^ ^ ^ < ^ H v v\n",
      "< ^ < H < > < v\n",
      "v H H ^ > ^ H v\n",
      "v H > ^ H > H v\n",
      "> ^ > H < < > G\n",
      "Epoch 19:\n",
      "best score: 0.61\n",
      "v v < v < < v v\n",
      "< < < < v v v ^\n",
      "v ^ > H v < v ^\n",
      "< < v v > H v v\n",
      "> ^ < H > > < v\n",
      "v H H ^ ^ < H v\n",
      "v H ^ > H ^ H v\n",
      "< ^ > H > < > G\n",
      "Epoch 20:\n",
      "best score: 0.62\n",
      "< v < v < v v v\n",
      "< < < < v v v v\n",
      "> v > H v < v v\n",
      "< ^ v < < H v v\n",
      "> > < H < > < v\n",
      "v H H > > v H v\n",
      "^ H < ^ H v H v\n",
      "> ^ > H v < > G\n",
      "Epoch 21:\n",
      "best score: 0.64\n",
      "< v ^ v ^ v v v\n",
      "< < < < < < v ^\n",
      "v > < H v < v v\n",
      "v ^ v < > H v ^\n",
      "< v < H v < < v\n",
      "> H H v > < H v\n",
      "v H < < H v H v\n",
      "> ^ ^ H < < > G\n",
      "Epoch 22:\n",
      "best score: 0.62\n",
      "< v v v < < v v\n",
      "< < < < v v v ^\n",
      "v v v H v < v v\n",
      "< ^ v > > H v v\n",
      "< ^ < H > > < v\n",
      "v H H > > < H v\n",
      "v H v > H > H v\n",
      "< ^ > H v v ^ G\n",
      "Epoch 23:\n",
      "best score: 0.63\n",
      "v v v v < v v v\n",
      "< < < < v v v ^\n",
      "v v > H v < v v\n",
      "< > v < > H v v\n",
      "< > < H > > < v\n",
      "v H H > ^ < H v\n",
      "> H v ^ H ^ H v\n",
      "> ^ > H v < ^ G\n",
      "Epoch 24:\n",
      "best score: 0.63\n",
      "< v v v < v v v\n",
      "< < < < < < v ^\n",
      "v ^ > H v < v v\n",
      "< ^ ^ < > H v v\n",
      "> v v H > < < v\n",
      "> H H > ^ ^ H v\n",
      "^ H < ^ H v H v\n",
      "> ^ > H v > > G\n",
      "Epoch 25:\n",
      "best score: 0.69\n",
      "< v ^ v ^ v v v\n",
      "< < < < < v v ^\n",
      "v ^ > H < < v v\n",
      "v < v < ^ H v v\n",
      "< > < H v < < v\n",
      "> H H v > v H v\n",
      "v H < < H > H v\n",
      "> ^ ^ H v < > G\n",
      "Epoch 26:\n",
      "best score: 0.71\n",
      "< v v v < v v v\n",
      "< < < < < v v ^\n",
      "v ^ > H v < v v\n",
      "< ^ v < > H v v\n",
      "> v < H > > < v\n",
      "^ H H v ^ < H v\n",
      "v H v > H v H v\n",
      "> ^ > H v < > G\n",
      "Epoch 27:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-403-0d8cf05b40a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#add new policies to the pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcrossovered\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmutated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mpool_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#select pool_size best policies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-403-0d8cf05b40a6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#add new policies to the pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcrossovered\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmutated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mpool_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#select pool_size best policies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-386-3d1488f5de0f>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(env, policy, n_times)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Run several evaluations and average the score the policy gets.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_random_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-386-3d1488f5de0f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Run several evaluations and average the score the policy gets.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_random_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-383-4251b8e6315b>\u001b[0m in \u001b[0;36msample_reward\u001b[1;34m(env, policy, t_max)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[1;34m(prob_n, np_random)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"initializing...\")\n",
    "np.random.seed(1234)\n",
    "pool = [get_random_policy() for i in range(pool_size)]\n",
    "pool_scores = [evaluate(env, policy) for policy in pool]\n",
    "\n",
    "\n",
    "assert type(pool) == type(pool_scores) == list\n",
    "assert len(pool) == len(pool_scores) == pool_size\n",
    "assert all([type(score) in (float, int) for score in pool_scores])\n",
    "\n",
    "\n",
    "#main loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch %s:\"%epoch)\n",
    "\n",
    "    crossovered = [crossover(pool[np.random.randint(len(pool))], pool[np.random.randint(len(pool))], p=0.5) for i in range(n_crossovers)]\n",
    "    mutated = [mutation(pol_cros, p=0.3) for pol_cros in crossovered]\n",
    "\n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "\n",
    "    #add new policies to the pool\n",
    "    pool += crossovered + mutated\n",
    "    pool_scores = [evaluate(env, policy) for policy in pool]\n",
    "\n",
    "    #select pool_size best policies\n",
    "    selected_indices = np.argsort(pool_scores)[-pool_size:]\n",
    "    pool = [pool[i] for i in selected_indices][-50:]\n",
    "    pool_scores = [pool_scores[i] for i in selected_indices]\n",
    "\n",
    "    #print the best policy so far (last in ascending score order)\n",
    "    print(\"best score:\", pool_scores[-1])\n",
    "    print_policy(pool[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пытаемся побить 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 100 #how many cycles to make\n",
    "pool_size = 200 #how many policies to maintain\n",
    "n_crossovers = 100 #how many crossovers to make on each step\n",
    "n_mutations = 100 #how many mutations to make on each tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing...\n",
      "Epoch 0:\n",
      "best score: 0.06\n",
      "^ < < < ^ ^ < v\n",
      "v ^ < < < > v ^\n",
      "< v < H < ^ v ^\n",
      "> ^ v v ^ H < >\n",
      "^ v v H > v < ^\n",
      "^ H H v > > H v\n",
      "> H > < H > H v\n",
      "> ^ > H ^ < ^ G\n",
      "Epoch 1:\n",
      "best score: 0.1\n",
      "^ < < < ^ ^ < v\n",
      "v ^ < < < > v ^\n",
      "< v < H < ^ v ^\n",
      "> ^ v v ^ H < >\n",
      "^ v v H > v < ^\n",
      "^ H H v > > H v\n",
      "> H > < H > H v\n",
      "> ^ > H ^ < ^ G\n",
      "Epoch 2:\n",
      "best score: 0.1\n",
      "^ < < < ^ ^ < v\n",
      "v ^ < < < > v ^\n",
      "< v < H < ^ v ^\n",
      "> ^ v v ^ H < >\n",
      "^ v v H > v < ^\n",
      "^ H H v > > H v\n",
      "> H > < H > H v\n",
      "> ^ > H ^ < ^ G\n",
      "Epoch 3:\n",
      "best score: 0.12\n",
      "v < v v v ^ > <\n",
      "> > > ^ > v v v\n",
      "> < ^ H < < ^ ^\n",
      "< v v > ^ H v v\n",
      "< ^ < H < < v v\n",
      "< H H > ^ v H v\n",
      "> H > ^ H > H v\n",
      "^ ^ > H < v v G\n",
      "Epoch 4:\n",
      "best score: 0.14\n",
      "^ > > v ^ v v v\n",
      "v < < < < > v >\n",
      "< v ^ H < ^ v v\n",
      "> v v v < H < v\n",
      "^ < ^ H < v < v\n",
      "v H H v > v H v\n",
      "^ H < ^ H v H v\n",
      "v > > H ^ > ^ G\n",
      "Epoch 5:\n",
      "best score: 0.2\n",
      "v > v v < v ^ v\n",
      "v v < < < > v v\n",
      "< < ^ H v ^ v v\n",
      "< v > v < H < v\n",
      "^ < < H ^ v < v\n",
      "< H H v > v H v\n",
      "v H v ^ H v H v\n",
      "> > > H ^ v < G\n",
      "Epoch 6:\n",
      "best score: 0.24\n",
      "< < < ^ v v v v\n",
      "> v < < < > v >\n",
      "< > > H < v v v\n",
      "v ^ > v v H v v\n",
      "< < ^ H v v ^ >\n",
      "< H H v < v H v\n",
      "^ H > ^ H v H v\n",
      "> v v H ^ > v G\n",
      "Epoch 7:\n",
      "best score: 0.33\n",
      "< < < v < v ^ v\n",
      "v ^ < < < > v v\n",
      "^ > ^ H v ^ v v\n",
      "< v > v v H ^ v\n",
      "^ < ^ H ^ v < v\n",
      "> H H v < v H v\n",
      "v H ^ ^ H v H v\n",
      "> > > H ^ < v G\n",
      "Epoch 8:\n",
      "best score: 0.29\n",
      "< < < v < v ^ v\n",
      "v ^ < < < > v v\n",
      "^ > ^ H v ^ v v\n",
      "< v > v v H ^ v\n",
      "^ < ^ H ^ v < v\n",
      "> H H v < v H v\n",
      "v H ^ ^ H v H v\n",
      "> > > H ^ < v G\n",
      "Epoch 9:\n",
      "best score: 0.4\n",
      "< < < < v v v v\n",
      "v v < v < v v v\n",
      "> > > H < ^ v v\n",
      "< > > v v H ^ v\n",
      "^ < v H v v > v\n",
      "< H H v < > H v\n",
      "^ H ^ ^ H > H v\n",
      "^ v > H v > > G\n",
      "Epoch 10:\n",
      "best score: 0.43\n",
      "< < ^ v < ^ ^ v\n",
      "> v < < < v v v\n",
      "< > > H v < v ^\n",
      "^ v ^ ^ > H v ^\n",
      "< ^ v H v ^ < v\n",
      "v H H < ^ < H v\n",
      "^ H < v H < H v\n",
      "> < v H ^ v v G\n",
      "Epoch 11:\n",
      "best score: 0.51\n",
      "< < < < v v v v\n",
      "v v ^ < < < ^ v\n",
      "v ^ < H v v v v\n",
      "< v ^ v ^ H v v\n",
      "^ < > H > < < v\n",
      "< H H > > v H v\n",
      "> H v ^ H < H v\n",
      "^ v v H < ^ v G\n",
      "Epoch 12:\n",
      "best score: 0.48\n",
      "< < ^ v < ^ ^ v\n",
      "> v < < < v v v\n",
      "< > > H v < v ^\n",
      "^ v ^ ^ > H v ^\n",
      "< ^ v H v ^ < v\n",
      "v H H < ^ < H v\n",
      "^ H < v H < H v\n",
      "> < v H ^ v v G\n",
      "Epoch 13:\n",
      "best score: 0.48\n",
      "< < v ^ v < v v\n",
      "> ^ < < < < ^ ^\n",
      "< > > H < ^ v ^\n",
      "v ^ < v < H v v\n",
      "< > < H < ^ < v\n",
      "< H H v < > H v\n",
      "v H v < H ^ H v\n",
      "< v v H ^ > v G\n",
      "Epoch 14:\n",
      "best score: 0.57\n",
      "< < < v v < v v\n",
      "v v < < < v v ^\n",
      "< ^ > H ^ v v v\n",
      "> > v v v H v v\n",
      "< v v H v < v v\n",
      "> H H v ^ > H v\n",
      "> H > v H v H v\n",
      "> > > H v v > G\n",
      "Epoch 15:\n",
      "best score: 0.56\n",
      "< < < v v v v ^\n",
      "^ < v < < < v v\n",
      "< ^ v H v < v ^\n",
      "< v < ^ ^ H v v\n",
      "< < ^ H v v < v\n",
      "v H H v < > H v\n",
      "v H > < H < H v\n",
      "< > < H < > < G\n",
      "Epoch 16:\n",
      "best score: 0.51\n",
      "< < < v < v v ^\n",
      "> > < < < < v ^\n",
      "> > > H v v v v\n",
      "v v < ^ ^ H < v\n",
      "v > < H < v < v\n",
      "< H H v ^ > H v\n",
      "^ H ^ < H ^ H v\n",
      "^ v < H v > v G\n",
      "Epoch 17:\n",
      "best score: 0.53\n",
      "< < ^ ^ v ^ v v\n",
      "> < < < < < v v\n",
      "v > > H v < v ^\n",
      "^ v < ^ < H v v\n",
      "^ ^ > H > < < v\n",
      "^ H H < ^ v H v\n",
      "^ H < v H < H v\n",
      "> < v H ^ v < G\n",
      "Epoch 18:\n",
      "best score: 0.59\n",
      "v < v v v v ^ >\n",
      "< < < < < v v v\n",
      "> > > H ^ < v ^\n",
      "< > ^ ^ v H v v\n",
      "< ^ ^ H > > < v\n",
      "^ H H ^ < > H v\n",
      "v H v v H v H v\n",
      "> ^ v H ^ v > G\n",
      "Epoch 19:\n",
      "best score: 0.55\n",
      "< < < < v v v v\n",
      "v v ^ < < < ^ v\n",
      "v ^ < H v v v v\n",
      "< v ^ v ^ H v v\n",
      "^ < > H > < < v\n",
      "< H H > > v H v\n",
      "> H v ^ H < H v\n",
      "^ v v H < ^ v G\n",
      "Epoch 20:\n",
      "best score: 0.7\n",
      "v < v v v v ^ v\n",
      "< < < < < v v v\n",
      "> > > H v < v ^\n",
      "> > ^ ^ < H v v\n",
      "< ^ ^ H v v < v\n",
      "^ H H ^ < > H v\n",
      "v H v v H v H v\n",
      "> ^ v H > v > G\n",
      "Epoch 21:\n",
      "best score: 0.63\n",
      "< < v v < v v v\n",
      "< < < < < < v ^\n",
      "v v < H v v v v\n",
      "> > ^ v < H v v\n",
      "< v > H < < < v\n",
      "< H H v > > H v\n",
      "> H ^ ^ H ^ H v\n",
      "v v > H v < ^ G\n",
      "Epoch 22:\n",
      "best score: 0.67\n",
      "< v v v < v v ^\n",
      "< < < < v v v >\n",
      "v > > H v < v ^\n",
      "< v ^ ^ < H v ^\n",
      "< < > H v v < v\n",
      "v H H < < > H v\n",
      "^ H > < H < H v\n",
      "> < ^ H < v < G\n",
      "Epoch 23:\n",
      "best score: 0.62\n",
      "< < v < v ^ v v\n",
      "< < < < < < ^ ^\n",
      "v > > H v v v ^\n",
      "^ v < v v H v v\n",
      "^ < > H v < < v\n",
      "v H H < ^ v H v\n",
      "^ H < v H ^ H v\n",
      "^ ^ v H < v v G\n",
      "Epoch 24:\n",
      "best score: 0.64\n",
      "< < v < v ^ v v\n",
      "> v < < < v v v\n",
      "< > > H v < v ^\n",
      "^ > < ^ v H v v\n",
      "< < > H v < v v\n",
      "^ H H v ^ v H v\n",
      "^ H < ^ H ^ H v\n",
      "v ^ v H > v < G\n",
      "Epoch 25:\n",
      "best score: 0.66\n",
      "< v v v v v v ^\n",
      "< < < < v v v >\n",
      "v > > H v < v ^\n",
      "< v ^ ^ < H v v\n",
      "< < ^ H v v < v\n",
      "v H H < < > H v\n",
      "v H > < H ^ H v\n",
      "< < ^ H < v < G\n",
      "Epoch 26:\n",
      "best score: 0.65\n",
      "< < < v v < v v\n",
      "v ^ < < < < v ^\n",
      "< > ^ H < > v ^\n",
      "v > < < < H v v\n",
      "< v < H v < < v\n",
      "v H H v < > H v\n",
      "^ H < v H ^ H v\n",
      "< v < H ^ v ^ G\n",
      "Epoch 27:\n",
      "best score: 0.63\n",
      "v < v v v v ^ v\n",
      "< < < < < v v v\n",
      "> > > H v < v ^\n",
      "> > ^ ^ < H v v\n",
      "< ^ ^ H v v < v\n",
      "^ H H ^ < > H v\n",
      "v H v v H v H v\n",
      "> ^ v H > v > G\n",
      "Epoch 28:\n",
      "best score: 0.66\n",
      "v v v v ^ v v >\n",
      "< < < < < v v v\n",
      "> > < H v < v ^\n",
      "v > ^ v ^ H v v\n",
      "< ^ < H ^ v < v\n",
      "^ H H v < > H v\n",
      "^ H < < H ^ H v\n",
      "< ^ v H v v v G\n",
      "Epoch 29:\n",
      "best score: 0.67\n",
      "< v v < v ^ v v\n",
      "< < < < < < v v\n",
      "< ^ < H v v v ^\n",
      "^ > > ^ v H v v\n",
      "< v > H ^ < < v\n",
      "v H H ^ ^ v H v\n",
      "^ H > v H ^ H v\n",
      "v ^ > H v v < G\n",
      "Epoch 30:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-405-984d9fb9f3c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#add new policies to the pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcrossovered\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmutated\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmutated_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mpool_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#select pool_size best policies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-405-984d9fb9f3c8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#add new policies to the pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcrossovered\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmutated\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmutated_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mpool_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#select pool_size best policies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-386-3d1488f5de0f>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(env, policy, n_times)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Run several evaluations and average the score the policy gets.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_random_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-386-3d1488f5de0f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Run several evaluations and average the score the policy gets.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_random_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-383-4251b8e6315b>\u001b[0m in \u001b[0;36msample_reward\u001b[1;34m(env, policy, t_max)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[1;34m(prob_n, np_random)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"initializing...\")\n",
    "np.random.seed(1234)\n",
    "pool = [get_random_policy() for i in range(pool_size)]\n",
    "pool_scores = [evaluate(env, policy) for policy in pool]\n",
    "\n",
    "\n",
    "assert type(pool) == type(pool_scores) == list\n",
    "assert len(pool) == len(pool_scores) == pool_size\n",
    "assert all([type(score) in (float, int) for score in pool_scores])\n",
    "\n",
    "\n",
    "#main loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch %s:\"%epoch)\n",
    "\n",
    "    crossovered = [crossover(pool[np.random.randint(len(pool))], pool[np.random.randint(len(pool))], p=0.7) for i in range(n_crossovers)]\n",
    "    mutated = [mutation(pol_cros, p=0.3) for pol_cros in crossovered]\n",
    "    mutated_old = [mutation(pol, p=0.3) for pol in pool]\n",
    "\n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "\n",
    "    #add new policies to the pool\n",
    "    pool += crossovered + mutated + mutated_old\n",
    "    pool_scores = [evaluate(env, policy) for policy in pool]\n",
    "\n",
    "    #select pool_size best policies\n",
    "    selected_indices = np.argsort(pool_scores)[-pool_size:]\n",
    "    pool = [pool[i] for i in selected_indices][-200:]\n",
    "    pool_scores = [pool_scores[i] for i in selected_indices]\n",
    "\n",
    "    #print the best policy so far (last in ascending score order)\n",
    "    print(\"best score:\", pool_scores[-1])\n",
    "    print_policy(pool[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не вышло =( попытаемся в другой раз =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus I (2 points):\n",
    "* Gym envs have a condition for \"beating the game\". E.g. here's the conditions for [Taxi-v1](https://gym.openai.com/envs/Taxi-v1). \n",
    "* If you managed to do that, it's worth uploading your first solution to gym. See `gym.upload(...)` docs. Allbeit it isn't a strong AI (or is it?), uploading your algorithm would be a good start. (and a +point!)\n",
    "* You'll get __+1 point__ for uploading and __+1 more if you beat the game__\n",
    "\n",
    "### Bonus II (4 points):\n",
    "* There are environments with continuous state spaces. In fact, most real world environments have this property. While we will dive into methods designed for that later, right now you already can solve them through binarization.\n",
    " * Gym has a basic infinite-state-space env called [CartPole](https://gym.openai.com/envs/CartPole-v0) - please start from this one. Solving something more challenging is great, but make sure your algorithm beats cartpole first. Also kudos for submitting.\n",
    " * Main idea: if you have something infinite and you want something discrete, you split it into bins. Like what histogram does.\n",
    " * Good choice of discretes is critical!\n",
    " * If the dimensionality is too high, you can try to reduce it (PCA/autoencoders)\n",
    "\n",
    "\n",
    "\n",
    "If you're running on a server/in binder, you may want to run this _at the very beginning of the notebook_ (before first cell imports gym):\n",
    "```\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
